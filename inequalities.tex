\documentclass{article}
\usepackage[utf8]{inputenc}

\usepackage{amssymb,amsfonts,amsthm,amsmath}
\usepackage[margin=1in]{geometry}
\usepackage[usenames,dvipsnames,svgnames]{xcolor}
\usepackage{hyperref,url}
\hypersetup{
    colorlinks=true,
    citecolor=MidnightBlue,
    urlcolor=Bittersweet,
}
\newcommand{\<}{\langle}
\renewcommand{\>}{\rangle}
\newcommand{\iprod}[2]{\left\langle #1 , #2 \right\rangle}

% Theorems
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}
\newtheorem{corollary}{Corollary}%[section]
\newtheorem{proposition}{Proposition}%[section]
\newtheorem{definition}{Definition}%[section] % number this the same as theorem and lemma




\title{Analysis Toolbox: Strong convexity and Lipschitz continuity of gradients}
\author{
Stephen Becker \\
Applied Math, U.\ Colorado Boulder
\texttt{stephen.becker@colorado.edu}
}
\date{Created Sep 27 2019, last edited \today}

\begin{document}

\maketitle

\subsection*{Lipschitz continuity of derivative and/or strong convexity of $f$}
The definition of Lipschitz continuity of $\nabla f$  (with constant $L$) is
\begin{equation} \label{eq:LL}
    \forall x, y\quad \| \nabla f(x) - \nabla f(y) \| \le L \|x-y\|
\end{equation}
and the definition of $f$ being $\mu$ strongly convex means that the function $x \mapsto f(x) - \frac{\mu}{2}\|x\|^2$ is convex. In the lines below, if $L$ or $\mu$ appears, then we are assuming the gradient is Lipschitz with constant $L$ or $f$ is strongly convex with constant $\mu$, respectively. Most references to Nesterov's book are to his first edition~\cite{Nesterov_2004}, not the recent 2018  edition~\cite{Nesterov_2018}.

These two inequalities are very helpful; see, e.g., Thm 2.1.5 and Thm 2.1.10 from \cite{Nesterov_2004}.
\begin{align}
    f(y) &\le f(x) + \iprod{\nabla f(x)}{ y - x } + \frac{L}{2}\|x-y\|^2 \label{eq:L} \\
    f(y) &\ge f(x) + \iprod{\nabla f(x)}{ y - x } + \frac{\mu}{2}\|x-y\|^2 \label{eq:mu}
\end{align}
If we drop convexity but keep Lipschitz continuity of the gradient, then the first equation is still true, but the second equation is not true with $\mu=0$, but it is true with $\mu = -L$.  This is often written as
$\left| f(y) - ( f(x) + \iprod{\nabla f(x)}{ y - x } )\right| \le \frac{L}{2}\|x-y\|^2$.

The main inequalities can be summarized by: 
% Not sure how to number; see https://tex.stackexchange.com/a/103894 for a way that won't work if I want this format
\begin{equation} \label{eq:big}
\left.\begin{aligned}
L^{-1} \| \nabla f(x) - \nabla f(y) \|^2  \quad\text{\small\textcolor{red}{(a)}} \\
\mu \|x-y\|^2 
\quad\text{\small\textcolor{red}{(b)}} \\
\frac{\mu L}{\mu + L} \|x-y\|^2  + \frac{1}{\mu+L}\| \nabla f(x) - \nabla f(y) \|^2
\quad\text{\small\textcolor{red}{(c)}}
\end{aligned} \right\rbrace
\le \< \nabla f(x) - \nabla f(y), x-y \> 
\le 
\left\lbrace\begin{aligned}
\text{\small\textcolor{red}{(d)}} \quad& L \|x-y\|^2  \\
\text{\small\textcolor{red}{(e)}} \quad& \mu^{-1}  \| \nabla f(x) - \nabla f(y) \|^2
\end{aligned} \right.
\end{equation}
The inequality {\small\textcolor{red}{(a)}} %left-most $\le$ above in the line for $L$ 
really follows from the co-coercivity of gradients; this result is actually surprisingly strong, since it makes implicit use of the Baillon-Haddad theorem. The result {\small\textcolor{red}{(e)}} for $\mu$ also requires $f$ be continuously differentiable. 
The {\small\textcolor{red}{(c)}} inequality 
assumes both strong convexity and Lipschitz continuity of the gradient; see \cite[Thm. 2.1.12]{Nesterov_2004} for a derivation.




\paragraph{Sub-optimality bounds} Added Sept 17 2019.  For unconstrained smooth optimization, if $x^\star$ is a minimizer, then $\nabla f(x^\star) = 0$. Note there are 3 equivalent definitions of optimality: $x$ is optimal if
\begin{equation}
    \|x - x^\star\|=0, \quad
    f(x) - f^\star = 0, \quad
    \|\nabla f(x)\| = 0
\end{equation}
and this would be ``iff'' if we assume the optimal solution is unique. Now, given a Lipschitz continuous derivative, we can bound
\begin{align}
 \|\nabla f(x) \| = \| \nabla f(x) - \nabla f(x^\star)\| &\le L \|x-x^\star\| \quad \text{by \eqref{eq:LL}}\\
 f(x) - f^\star &\le \frac{L}{2}\|x-x^\star\|^2
 \quad\text{by \eqref{eq:L}} \\
 \|\nabla f(x) \|^2 &\le 2L \left( f(x) - f^\star\right) \quad\text{by Eq. (9.14) in \cite{BoydVandenbergheBook}} \label{eq:33}
\end{align}
and given $\mu$ strong convexity, we can bound in the other direction:
\begin{align}
\|x-x^\star\|^2 &\le \frac{1}{\mu^2}\|\nabla f(x)\|^2 
\quad \text{by \eqref{eq:big}  {\small\textcolor{red}{(b)}} and  {\small\textcolor{red}{(e)}} }
\\
\|x-x^\star\|^2 &\le \frac{2}{\mu}\left( f(x) - f^\star\right)
\quad \text{by \eqref{eq:mu}, with $x=x^\star$, $y=x$} \\
f(x) - f^\star &\le \frac{1}{2\mu}\|\nabla f(x)\|^2 \quad\text{by Eq. (9.9) in \cite{BoydVandenbergheBook}. This is PL}
\label{eq:44}
\end{align}
Given both $L$ and $\mu$, we can combine the bounds, and bound any one of the 3 error metrics in terms of another, i.e.,
%\begin{align*}
 $\|\nabla f(x) \|^2 \le \frac{2L^2}{\mu}\left( f(x) - f^\star\right)$ and $% \\
 f(x) - f^\star \le \frac{L}{2\mu^2}\|\nabla f(x)\|^2. $
%\end{align*}.
But these are not good bounds; the bounds in Eq~\eqref{eq:33} and \eqref{eq:44} are better. Note: \eqref{eq:44} is the Polyak-Lojasiewicz (PL) inequality, see \href{https://arxiv.org/abs/1608.04636}{Karimi, Nutini, Schmidt} for details.


\begin{thebibliography}{AZQRY16}

\bibitem[BC11]{bauschke2011convex}
H. H. Bauschke and P.~L. Combettes, \href{https://link.springer.com/book/10.1007/978-1-4419-9467-7}{\emph{Convex analysis and monotone
  operator theory in {H}ilbert spaces}, 1st edition}, {S}pringer, 2011.
  
\bibitem[BC17]{bauschke2017convex}
H. H. Bauschke and P.~L. Combettes, \href{https://link.springer.com/book/10.1007/978-3-319-48311-5}{\emph{Convex analysis and monotone
  operator theory in {H}ilbert spaces}, 2nd edition}, {S}pringer, 2017.

\bibitem[BV04]{BoydVandenbergheBook}
S.~Boyd and L.~Vandenberghe.
\newblock \href{http://www.stanford.edu/~boyd/cvxbook/}{{\em Convex Optimization}}.
\newblock Cambridge University Press, 2004.

\bibitem[Nes04]{Nesterov_2004}
Yu.~Nesterov.
\newblock \href{https://link.springer.com/book/10.1007/978-1-4419-8853-9}{{\em Introductory Lectures on Convex Optimization: A Basic Course}},
  volume~87 of {\em Applied Optimization}.
\newblock Kluwer, Boston, 2004.

\bibitem[Nes18]{Nesterov_2018}
Yu. Nesterov.
\newblock \href{https://link.springer.com/book/10.1007/978-3-319-91578-4}{{\em Lectures on Convex Optimization}}.
\newblock Springer International Publishing, 2018.

\end{thebibliography}

\end{document}
